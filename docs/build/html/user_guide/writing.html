<!DOCTYPE html>


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>Writing Data &mdash; OpenTDSB 2.0 documentation</title>
    
    <link rel="stylesheet" href="../_static/solar.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../',
        VERSION:     '2.0',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <link rel="top" title="OpenTDSB 2.0 documentation" href="../index.html" />
    <link rel="up" title="User Guide" href="index.html" />
    <link rel="next" title="Querying or Reading Data" href="query/index.html" />
    <link rel="prev" title="Configuration" href="configuration.html" /><link href='http://fonts.googleapis.com/css?family=Source+Code+Pro|Open+Sans:300italic,400italic,700italic,400,300,700' rel='stylesheet' type='text/css'>
<link href="../_static/solarized-dark.css" rel="stylesheet">
<script type="text/javascript">
  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-18339382-1']);
  _gaq.push(['_setDomainName', 'none']);
  _gaq.push(['_setAllowLinker', true]);
  _gaq.push(['_trackPageview']);
  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();
</script>
  </head>
  <body>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="query/index.html" title="Querying or Reading Data"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="configuration.html" title="Configuration"
             accesskey="P">previous</a> |</li>
        <li><a href="../index.html">OpenTDSB 2.0 documentation</a> &raquo;</li>
          <li><a href="index.html" accesskey="U">User Guide</a> &raquo;</li> 
      </ul>
    </div>
      <div class="sphinxsidebar">
        <div class="sphinxsidebarwrapper">
  <h3><a href="../index.html">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">Writing Data</a><ul>
<li><a class="reference internal" href="#naming-schema">Naming Schema</a></li>
<li><a class="reference internal" href="#data-specification">Data Specification</a><ul>
<li><a class="reference internal" href="#timestamps">Timestamps</a></li>
<li><a class="reference internal" href="#metrics-and-tags">Metrics and Tags</a></li>
<li><a class="reference internal" href="#integer-values">Integer Values</a></li>
<li><a class="reference internal" href="#floating-point-values">Floating Point Values</a></li>
<li><a class="reference internal" href="#ordering">Ordering</a></li>
</ul>
</li>
<li><a class="reference internal" href="#input-methods">Input Methods</a><ul>
<li><a class="reference internal" href="#telnet">Telnet</a></li>
<li><a class="reference internal" href="#http-api">Http API</a></li>
<li><a class="reference internal" href="#batch-import">Batch Import</a></li>
</ul>
</li>
<li><a class="reference internal" href="#write-performance">Write Performance</a><ul>
<li><a class="reference internal" href="#uid-assignment">UID Assignment</a></li>
<li><a class="reference internal" href="#pre-split-hbase-regions">Pre-Split HBase Regions</a></li>
<li><a class="reference internal" href="#distributed-hbase">Distributed HBase</a></li>
<li><a class="reference internal" href="#multiple-tsds">Multiple TSDs</a></li>
<li><a class="reference internal" href="#persistent-connections">Persistent Connections</a></li>
<li><a class="reference internal" href="#disable-meta-data-and-real-time-publishing">Disable Meta Data and Real Time Publishing</a></li>
</ul>
</li>
</ul>
</li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="configuration.html"
                        title="previous chapter">Configuration</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="query/index.html"
                        title="next chapter">Querying or Reading Data</a></p>
<div id="searchbox" style="display: none">
  <h3>Quick search</h3>
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body">
            
  <div class="section" id="writing-data">
<h1>Writing Data</h1>
<p>You may want to jump right in and start throwing data into your TSD, but to really take advantage of OpenTSDB&#8217;s power and flexibility, you may want to pause and think about your naming schema. After you&#8217;ve done that, you can procede to pushing data over the Telnet or HTTP APIs, or use an existing tool with OpenTSDB support such as &#8216;tcollector&#8217;.</p>
<div class="section" id="naming-schema">
<h2>Naming Schema</h2>
<p>Many metrics administrators are used to supplying a single name for their time series. For example, systems administrators used to RRD style systems may name their time series <tt class="docutils literal"><span class="pre">webserver01.sys.cpu.0.user</span></tt>. The name tells us that the time series is recording the amount of time in user space for cpu <tt class="docutils literal"><span class="pre">0</span></tt> on <tt class="docutils literal"><span class="pre">webserver01</span></tt>. This works great if you want to retrieve just the user time for that cpu core on that particular web server later on.</p>
<p>But what if the web server has 64 cores and you want to get the average time across all of them? Some systems allow you to specify a wild card such as <tt class="docutils literal"><span class="pre">webserver01.sys.cpu.*.user</span></tt> that would read all 64 files and aggregate the results. Alternatively, you could record a new time series called <tt class="docutils literal"><span class="pre">webserver01.sys.cpu.user.all</span></tt> that represents the same aggregate but you must now write &#8216;64 + 1&#8217; different time series. What if you had a thousand web servers and you wanted the average cpu time for all of your servers? You could craft a wild card query like <tt class="docutils literal"><span class="pre">*.sys.cpu.*.user</span></tt> and the system would open all 64,000 files, aggregate the results and return the data. Or you setup a process to pre-aggregate the data and write it to <tt class="docutils literal"><span class="pre">webservers.sys.cpu.user.all</span></tt>.</p>
<p>OpenTSDB handles things a bit differently by introducing the idea of &#8216;tags&#8217;. Each time series still has a &#8216;metric&#8217; name, but it&#8217;s much more generic, something that can be shared by many unique time series. Instead, the uniqueness comes from a combination of tag key/value pairs that allows for flexible queries with very fast aggregations.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Every time series in OpenTSDB must have at least one tag.</p>
</div>
<p>Take the previous example where the metric was <tt class="docutils literal"><span class="pre">webserver01.sys.cpu.0.user</span></tt>. In OpenTSDB, this may become <tt class="docutils literal"><span class="pre">sys.cpu.user</span> <span class="pre">host=webserver01,</span> <span class="pre">cpu=0</span></tt>. Now if we want the data for an individual core, we can craft a query like <tt class="docutils literal"><span class="pre">sum:sys.cpu.user{host=webserver01,cpu=42}</span></tt>. If we want all of the cores, we simply drop the cpu tag and ask for <tt class="docutils literal"><span class="pre">sum:sys.cpu.user{host=webserver01}</span></tt>. This will give us the aggregated results for all 64 cores. If we want the results for all 1,000 servers, we simply request <tt class="docutils literal"><span class="pre">sum:sys.cpu.user</span></tt>. The underlying data schema will store all of the <tt class="docutils literal"><span class="pre">sys.cpu.user</span></tt> time series next to each other so that aggregating the individual values is very fast and efficient. OpenTSDB was designed to make these aggregate queries as fast as possible since most users start out at a high level, then drill down for detailed information.</p>
<p>While the tagging system is flexible, some problems can arise if you don&#8217;t understand how the querying side of OpenTSDB, hence the need for some forethought. Take the example query above: <tt class="docutils literal"><span class="pre">sum:sys.cpu.user{host=webserver01}</span></tt>. We recorded 64 unique time series for <tt class="docutils literal"><span class="pre">webserver01</span></tt>, one time series for each of the CPU cores. When we issues that query, all of the time series for metric <tt class="docutils literal"><span class="pre">sys.cpu.user</span></tt> with the tag <tt class="docutils literal"><span class="pre">host=webserver01</span></tt> were retrieved, averaged and returned as one series of numbers. Lets say the resulting average was <tt class="docutils literal"><span class="pre">50</span></tt> for timestamp <tt class="docutils literal"><span class="pre">1356998400</span></tt>. Now we were migrating from another system to OpenTSDB and had a process that pre-aggregated all 64 cores so that we could quickly get the average value and simply wrote a new time series &#8220;sys.cpu.user host=webserver01``. If we run the same query, we&#8217;ll get a value of <tt class="docutils literal"><span class="pre">100</span></tt> at <tt class="docutils literal"><span class="pre">1356998400</span></tt>. What happened? OpenTSDB aggregated all 64 time series &#8216;and&#8217; the pre-aggregated time series to get to that 100. In storage, we would have something like this:</p>
<div class="highlight-python"><div class="highlight"><pre>sys.cpu.user host=webserver01        1356998400  50
sys.cpu.user host=webserver01,cpu=0  1356998400  1
sys.cpu.user host=webserver01,cpu=1  1356998400  0
sys.cpu.user host=webserver01,cpu=2  1356998400  2
sys.cpu.user host=webserver01,cpu=3  1356998400  0
...
sys.cpu.user host=webserver01,cpu=63 1356998400  1
</pre></div>
</div>
<p>OpenTSDB will &#8216;automatically&#8217; aggregate &#8216;all&#8217; of the time series for the metric in a query if no tags are given. If one or more tags are definied, the aggregate will &#8216;include all&#8217; time series that match on that tag, regardless of other tags. With the query <tt class="docutils literal"><span class="pre">sum:sys.cpu.user{host=webserver01}</span></tt>, we would include <tt class="docutils literal"><span class="pre">sys.cpu.user</span> <span class="pre">host=webserver01,cpu=0</span></tt> as well as <tt class="docutils literal"><span class="pre">sys.cpu.user</span> <span class="pre">host=webserver01,cpu=0,manufacturer=Intel</span></tt>, <tt class="docutils literal"><span class="pre">sys.cpu.user</span> <span class="pre">host=webserver01,foo=bar</span></tt> and <tt class="docutils literal"><span class="pre">sys.cpu.user</span> <span class="pre">host=webserver01,cpu=0,datacenter=lax,department=ops</span></tt>. The moral of this example is: be careful about your naming schema.</p>
<p>While OpenTSDB&#8217;s aggregations are pretty fast, sometimes they aren&#8217;t fast enough and you may indeed want to pre-aggregate something. For example, if you have many thousands of hosts, it would take a while to aggregate the CPU time across all of those servers. If you want to record and query just the pre-aggregated value separately, simply use a different metric. For example: <tt class="docutils literal"><span class="pre">sys.cpu.user.avg</span> <span class="pre">host=webserver01</span></tt> or <tt class="docutils literal"><span class="pre">sys.cpu.user.sum</span> <span class="pre">host=webserver01</span></tt>. Now you can query the pre-aggregate for just that one server or you could even aggregate the pre-aggregates for all of your servers with <tt class="docutils literal"><span class="pre">avg:sys.cpu.user.avg</span></tt>.</p>
<p>When you design your naming schema, keep these in mind:</p>
<ul class="simple">
<li>Be consistent with your naming</li>
<li>Use the same number and type of tags for each metric</li>
<li>Think about the most common queries you&#8217;ll be executing</li>
<li>Think about how you may want to drill down when querying</li>
<li>Don&#8217;t use too many tags, keep it to a fairly small number</li>
</ul>
</div>
<div class="section" id="data-specification">
<h2>Data Specification</h2>
<p>Every time series data point requires the following data:</p>
<ul class="simple">
<li>metric - A generic name for the timeseries such as <tt class="docutils literal"><span class="pre">sys.cpu.user</span></tt>, <tt class="docutils literal"><span class="pre">stock.quote</span></tt> or <tt class="docutils literal"><span class="pre">env.probe.temp</span></tt>.</li>
<li>timestamp - A Unix/POSIX Epoch timestamp in seconds or milliseconds defined as the number of seconds that have elapsed since January 1st, 1970 at 00:00:00 UTC time.</li>
<li>value - A numeric value to store at the given timestamp for the time series. This may be an integer or a floating point value.</li>
<li>tag(s) - A key/value pair consisting of a <tt class="docutils literal"><span class="pre">tagk</span></tt> (the key) and a <tt class="docutils literal"><span class="pre">tagv</span></tt> (the value). Each data point must have at least one tag.</li>
</ul>
<div class="section" id="timestamps">
<h3>Timestamps</h3>
<p>Data can be written to OpenTSDB with second or millisecond resolution. When writing to the telnet interface, millisecond timestamps may be of the format <tt class="docutils literal"><span class="pre">1364410924250</span></tt> where three digits are appended that represent the milliseconds, or <tt class="docutils literal"><span class="pre">1364410924.250</span></tt> where three digits are placed after a period. Timestamps with more than 13 digits will throw an error, so if your application outputs timestamps with more than three millisecond digits, you must round the value before submitting it. Timestamps sent to the <tt class="docutils literal"><span class="pre">/api/put</span></tt> endpoint over HTTP must be integers and may not have periods.</p>
<p>Timestamps with second resolution are stored on 2 bytes while millisecond resolution are stored on 4. Thus if you do not need millisecond resolution or all of your data points are on 1 second boundaries, we recommend that you submit timestamps with 10 digits for second resolution so that you can save on storage space. It&#8217;s also a good idea to avoid mixing second and millisecond timestamps for a given time series. Doing so will slow down queries as iteration across mixed timestamps takes longer than if you only record one type or the other. OpenTSDB will store whatever you give it.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Providing millisecond resolution does not necessarily mean that OpenTSDB supports write speeds of 1 data point per millisecond over many time series. While a single TSD may be able to handle a few thousand writes per second, that would only cover a few time series if you&#8217;re try to store a point every millisecond. Instead OpenTSDB aims to provide greater measurement accuracy and you should generally avoid recording data at such a speed, particularly for long running time series.</p>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Data with millisecond resolution can only be extracted via the <tt class="docutils literal"><span class="pre">/api/query</span></tt> endpoint at this time. See <a class="reference internal" href="query/index.html"><em>Querying or Reading Data</em></a> for details.</p>
</div>
</div>
<div class="section" id="metrics-and-tags">
<h3>Metrics and Tags</h3>
<p>The following rules apply to metric and tag values:</p>
<ul class="simple">
<li>Strings are case sensitive, i.e. &#8220;Sys.Cpu.User&#8221; will be stored separately from &#8220;sys.cpu.user&#8221;</li>
<li>Spaces are not allowed</li>
<li>Only the following characters are allowed: <tt class="docutils literal"><span class="pre">a</span></tt> to <tt class="docutils literal"><span class="pre">z</span></tt>, <tt class="docutils literal"><span class="pre">A</span></tt> to <tt class="docutils literal"><span class="pre">Z</span></tt>, <tt class="docutils literal"><span class="pre">0</span></tt> to <tt class="docutils literal"><span class="pre">9</span></tt>, <tt class="docutils literal"><span class="pre">-</span></tt>, <tt class="docutils literal"><span class="pre">_</span></tt>, <tt class="docutils literal"><span class="pre">.</span></tt>, <tt class="docutils literal"><span class="pre">/</span></tt> or Unicode letters (as per the specification)</li>
</ul>
<p>Metric and tags are not limited in length, though you should try to keep the values fairly short.</p>
</div>
<div class="section" id="integer-values">
<h3>Integer Values</h3>
<p>If the value from a <tt class="docutils literal"><span class="pre">put</span></tt> command is parsed without a decimal point, it will be treated as a signed integer. Currently all integers are stored on 8 bytes with planned support for variable length encoding in the future. This means a data point can have a minimum value of -9,223,372,036,854,775,808 and a maximum value of 9,223,372,036,854,775,807 (inclusive). Integers cannot have commas or any character other than digits and the dash (for negative values). E.g. if you wanted to store the maximum value, it would have to be provided in the form <tt class="docutils literal"><span class="pre">9223372036854775807</span></tt>.</p>
</div>
<div class="section" id="floating-point-values">
<h3>Floating Point Values</h3>
<p>If the value from a <tt class="docutils literal"><span class="pre">put</span></tt> command is parsed with a decimal point (represented by a period, <tt class="docutils literal"><span class="pre">.</span></tt>) it will be treated as a floating point value. Currently all floating point values are stored on 4 bytes, single-precision, with planned support for 8 bytes in the future. Floats are stored in IEEE 754 floating-point &#8220;single format&#8221; with positive and negative value support. Initinite and Not-a-Number values are not supported and will throw an error if supplied to a TSD. See <a class="reference external" href="https://en.wikipedia.org/wiki/IEEE_floating_point">Wikipedia</a> and the <a class="reference external" href="http://docs.oracle.com/javase/specs/jls/se7/html/jls-4.html#jls-4.2.3">Java Documentation</a> for details.</p>
</div>
<div class="section" id="ordering">
<h3>Ordering</h3>
<p>A great feature of OpenTSDB is that you can write data for a given time seriesin any order you want. You&#8217;re not limited to only storing data later than the last data point you wrote. This means you can start pushing current data from your systems to a TSD and then batch import historical data if you want to.</p>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">The only caveat when writing is that you cannot overwrite an existing value with a different value. Writing is idempotent, meaning you can write the value <tt class="docutils literal"><span class="pre">42</span></tt> at timestamp <tt class="docutils literal"><span class="pre">1356998400</span></tt> and then write <tt class="docutils literal"><span class="pre">42</span></tt> again for the same time, nothing bad will happen. However if you try to write <tt class="docutils literal"><span class="pre">42.5</span></tt> to the same timestamp, the row of data will become invalid (due to vagaries of the underlying schema) and any queries that include that row will throw an exception. Use the <tt class="docutils literal"><span class="pre">fsck</span></tt> utility to fix the row if this happens.</p>
</div>
</div>
</div>
<div class="section" id="input-methods">
<h2>Input Methods</h2>
<p>There are currently three main methods to get data into OpenTSDB: Telnet API, HTTP API and batch import from a file. Alternatively you can use a tool that provides OpenTSDB support, or if you&#8217;re extremely adventurous, use the Java library.</p>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">Don&#8217;t try to write directly to the underlying storage system, e.g. HBase. Just don&#8217;t. It&#8217;ll get messy quickly.</p>
</div>
<div class="section" id="telnet">
<h3>Telnet</h3>
<p>The easiest way to get started with OpenTSDB is to open up a terminal or telnet client, connect to your TSD and issue a <tt class="docutils literal"><span class="pre">put</span></tt> command and hit &#8216;enter&#8217;. If you are writing a program, simply open a socket, print the string command with a new line and send the packet. The telnet command format is:</p>
<div class="highlight-python"><div class="highlight"><pre>put &lt;metric&gt; &lt;timestamp&gt; &lt;value&gt; &lt;tagk1=tagv1[ tagk2=tagv2 ...tagkN=tagvN]&gt;
</pre></div>
</div>
<p>For example:</p>
<div class="highlight-python"><div class="highlight"><pre>put sys.cpu.user 1356998400 42.5 host=webserver01 cpu=0
</pre></div>
</div>
<p>Each <tt class="docutils literal"><span class="pre">put</span></tt> can only send a single data point. Don&#8217;t forget the newline character, e.g. <tt class="docutils literal"><span class="pre">\n</span></tt> at the end of your command.</p>
</div>
<div class="section" id="http-api">
<h3>Http API</h3>
<p>As of version 2.0, data can be sent over HTTP in formats supported by &#8216;Serializer&#8217; plugins. Multiple, un-related data points can be sent in a single HTTP POST request to save bandwidth. See the <a class="reference internal" href="../api_http/put.html"><em>/api/put</em></a> for details.</p>
</div>
<div class="section" id="batch-import">
<h3>Batch Import</h3>
<p>If you are importing data from another system or you need to backfill historical data, you can use the <tt class="docutils literal"><span class="pre">import</span></tt> CLI utility. See <a class="reference internal" href="cli/import.html"><em>import</em></a> for details.</p>
</div>
</div>
<div class="section" id="write-performance">
<h2>Write Performance</h2>
<p>OpenTSDB can scale to writing millions of data points per &#8216;second&#8217; on commodity servers with regular spinning hard drives. However users who fire up a VM with HBase in stand-alone mode and try to slam millions of data points at a brand new TSD are disappointed when they can only write data in the hundreds of points per second. Here&#8217;s what you need to do to scale for brand new installs or testing and for expanding existing systems.</p>
<div class="section" id="uid-assignment">
<h3>UID Assignment</h3>
<p>The first sticking point folks run into is &#8216;&#8217;uid assignment&#8217;&#8216;. Every string for a metric, tag key and tag value must be assigned a UID before the data point can be stored. For example, the metric <tt class="docutils literal"><span class="pre">sys.cpu.user</span></tt> may be assigned a UID of <tt class="docutils literal"><span class="pre">000001</span></tt> the first time it is encountered by a TSD. This assignment takes a fair amount of time as it must fetch an available UID, write a UID to name mapping and a name to UID mapping, then use the UID to write the data point&#8217;s row key. The UID will be stored in the TSD&#8217;s cache so that the next time the same metric comes through, it can find the UID very quickly.</p>
<p>Therefore, we recommend that you &#8216;pre-assign&#8217; uids to as many metrics, tag keys and tag values as you can. If you have designed a naming schema as recommended above, you&#8217;ll know most of the values to assign. You can use the CLI tools <a class="reference internal" href="cli/mkmetric.html"><em>mkmetric</em></a>, <a class="reference internal" href="cli/uid.html"><em>uid</em></a> or the HTTP API <a class="reference internal" href="../api_http/uid/index.html"><em>/api/uid</em></a> to perform pre-assignments. Any time you are about to send a bunch of new metrics or tags to a running OpenTSDB cluster, try to pre-assign or the TSDs will bog down a bit when they get the new data.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">If you restart a TSD, it will have to lookup the UID for every metric and tag so performance will be a little slow until the cache is filled.</p>
</div>
</div>
<div class="section" id="pre-split-hbase-regions">
<h3>Pre-Split HBase Regions</h3>
<p>For brand new installs you will see much better performance if you pre-split the regions in HBase regardless of if you&#8217;re testing on a stand-alone server or running a full cluster. HBase regions handle a defined range of row keys and are essentially a single file. When you create the <tt class="docutils literal"><span class="pre">tsdb</span></tt> table and start writing data for the first time, all of those data points are being sent to this one file on one server. As a region fills up, HBase will automatically split it into different files and move it to other servers in the cluster, but when this happens, the TSDs cannot write to the region and must buffer the data points. Therefore, if you can pre-allocate a number of regions before you start writing, the TSDs can send data to multiple files or servers and you&#8217;ll be taking advantage of the linear scalability immediately.</p>
<p>A simplest way to pre-split your <tt class="docutils literal"><span class="pre">tsdb</span></tt> table regions is to estimate the number of unique metric names you&#8217;ll be recording. If you have designed a naming schema, you should have a pretty good idea. Lets say we&#8217;ll track 4,000 metrics in our system. That&#8217;s not to say 4,000 time series, as we&#8217;re not counting the tags yet, just the metric names such as &#8220;sys.cpu.user&#8221;. Data points are written in row keys where the metric&#8217;s UID comprises the first bytes, 3 bytes by default. The first metric will be assigned a UID of <tt class="docutils literal"><span class="pre">000001</span></tt> as a hex encoded value. The 4,000th metric will have a UID of <tt class="docutils literal"><span class="pre">000FA0</span></tt> in hex. You can use these as the start and end keys in the script from the <a class="reference external" href="http://hbase.apache.org/book/perf.writing.html">HBase Book</a> to split your table into any number of regions. 256 regions may be a good place to start depending on how many time series share each metric.</p>
<p>TODO - include scripts for pre-splitting.</p>
<p>The simple split method above assumes that you have roughly an equal number of time series per metric (i.e. a fairly consistent cardinality). E.g. the metric with a UID of <tt class="docutils literal"><span class="pre">000001</span></tt> may have 200 time series and <tt class="docutils literal"><span class="pre">000FA0</span></tt> has about 150. If you have a wide range of time series per metric, e.g. <tt class="docutils literal"><span class="pre">000001</span></tt> has 10,000 time series while <tt class="docutils literal"><span class="pre">000FA0</span></tt> only has 2, you may need to develop a more complex splitting algorithm.</p>
<p>But don&#8217;t worry too much about splitting. As stated above, HBase will automatically split regions for you so over time, the data will be distributed fairly evenly.</p>
</div>
<div class="section" id="distributed-hbase">
<h3>Distributed HBase</h3>
<p>HBase will run in stand-alone mode where it will use the local file system for storing files. It will still use multiple regions and perform as well as the underlying disk or raid array will let it. You&#8217;ll definitely want a RAID array under HBase so that if a drive fails, you can replace it without losing data. This kind of setup is fine for testing or very small installations and you should be able to get into the low thousands of data points per second.</p>
<p>However if you want serious throughput and scalability you have to setup a Hadoop and HBase cluster with multiple servers. In a distributed setup HDFS manages region files, automatically distributing copies to different servers for fault tolerance. HBase assigns regions to different servers and OpenTSDB&#8217;s client will send data points to the specific server where they will be stored. You&#8217;re now spreading operations amongst multiple servers, increasing performance and storage. If you need even more throughput or storage, just add nodes or disks.</p>
<p>There are a number of ways to setup a Hadoop/HBase cluster and a ton of various tuning tweaks to make, so Google around and ask user groups for advice. Some general recomendations include:</p>
<ul class="simple">
<li>Dedicate a pair of high memory, low disk space servers for the Name Node. Set them up for high availability using something like Heartbeat and Pacemaker.</li>
<li>Setup Zookeeper on at least 3 servers for fault tolerance. They must have a lot of RAM and a fairly fast disk for log writing. On small clusters, these can run on the Name node servers.</li>
<li>JBOD for the HDFS data nodes</li>
<li>HBase region servers can be collocated with the HDFS data nodes</li>
<li>At least 1 gbps links between servers, 10 gbps preferable.</li>
<li>Keep the cluster in a single data center</li>
</ul>
</div>
<div class="section" id="multiple-tsds">
<h3>Multiple TSDs</h3>
<p>A single TSD can handle thousands of writes per second. But if you have many sources it&#8217;s best to scale by running multiple TSDs and using a load balancer (such as Varnish or DNS round robin) to distribute the writes. Many users colocate TSDs on their HBase region servers when the cluster is dedicated to OpenTSDB.</p>
</div>
<div class="section" id="persistent-connections">
<h3>Persistent Connections</h3>
<p>Enable keep alives in the TSDs and make sure that any applications you are using to send time series data keep their connections open instead of opening and closing for every write. See <a class="reference internal" href="configuration.html"><em>Configuration</em></a> for details.</p>
</div>
<div class="section" id="disable-meta-data-and-real-time-publishing">
<h3>Disable Meta Data and Real Time Publishing</h3>
<p>OpenTSDB 2.0 introduced meta data for tracking the kinds of data in the system. When tracking is enabled, a counter is incremented for every data point written and new UIDs or time series will generate meta data. The data may be pushed to a search engine or passed through tree generation code. These processes require greater memory in the TSD and may affect throughput. Tracking is disabled by default so test it out before enabling the feature.</p>
<p>2.0 also introduced a real-time publishing plugin where incoming data points can be emitted to another destination immediately after they&#8217;re queued for storage. This is diabled by default so test any plugins you are interested in before deploying in production.</p>
</div>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="query/index.html" title="Querying or Reading Data"
             >next</a> |</li>
        <li class="right" >
          <a href="configuration.html" title="Configuration"
             >previous</a> |</li>
        <li><a href="../index.html">OpenTDSB 2.0 documentation</a> &raquo;</li>
          <li><a href="index.html" >User Guide</a> &raquo;</li> 
      </ul>
    </div>
    <div class="footer">
        &copy; Copyright 2013, OpenTSDB.
    Created using <a href="http://sphinx.pocoo.org/">Sphinx</a>. Theme by <a href="http://github.com/vkvn">vkvn</a>
    </div>
  </body>
</html>